{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12724b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipython matplotlib seaborn numpy pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c550a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import typing\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import IPython.display as ipd                               \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# Konfiguration\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Pfade aufsetzen\n",
    "output = Path(f\"output\")\n",
    "output.mkdir(exist_ok=True)\n",
    "input = Path('input')\n",
    "if not input.exists() or next(input.iterdir(), None) is None:\n",
    "    raise Exception(f\"Expected input data in {input.name} not found\")\n",
    "workdir = Path(\"workdir\")\n",
    "if workdir.is_dir():\n",
    "    shutil.rmtree(workdir)\n",
    "workdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Standorte; Reihenfolge nach Priorität \n",
    "# d.h. Originale werden immer aus dem Standort\n",
    "# genommen, der hier zuerst steht \n",
    "AMS = \"AMS\"\n",
    "locations = [AMS, 'MZ', 'STG', 'BAD',  'KA', 'TUE', 'MA', 'FR']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguration: CSV einlesen\n",
    "\n",
    "na_values = [\n",
    "    '0\\'00000\"',\n",
    "    '',\n",
    "    'UNBEKANNT',\n",
    "    'Unbekannt',\n",
    "    '-',\n",
    "    'o. A',\n",
    "    'o.A.',\n",
    "    # BNR: Platzhalter-Werte\n",
    "    '[]',\n",
    "    '[-]', \n",
    "    '[unbekannt]',\n",
    "    '[PROMO]',\n",
    "    '[- PROMO]',\n",
    "    '[[ PROMO]]',\n",
    "    '[ PROMO]',\n",
    "    '[Promo]',\n",
    "    '[Promo-CD]',\n",
    "    '[o. A.]',\n",
    "    '[ohne Nummer]',\n",
    "    '[ohne Nr.]',\n",
    "    '[o. Nr.]'\n",
    "    # RHTI: Leere Werte\n",
    "    ' \"',\n",
    "    ' \" ',\n",
    "    # MIT: Leere Werte\n",
    "    # 'Diverse',\n",
    "    'Nicht genannt',\n",
    "    'nicht genannt'\n",
    "]\n",
    "\n",
    "# ordered by preference\n",
    "black_list_mit = [['Ensemble', 'Chor', 'Diverse', 'Orchester', 'Original Cast'], ['u.a.']]\n",
    "\n",
    "\"\"\"\n",
    "Sonstige, überdurchschnittlich häufige Werte, ggf. auch als leer interpretieren?\n",
    "    # RHTI: Allweltstitel \n",
    "    'Live',\n",
    "    'Streichquartette',\n",
    "    'Lieder',\n",
    "    'Kammermusik',\n",
    "    'Live USA',\n",
    "    'Greatest hits', \n",
    "    'Greatest Hits',\n",
    "    'Die großen Erfolge',\n",
    "    # MIT: Gruppenbezeichnungen\n",
    "    'Diverse',\n",
    "    'Ensemble',\n",
    "    'Orchester',\n",
    "    'Original Cast'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for value in list(na_values):\n",
    "    na_values.append(value.lower())\n",
    "    na_values.append(value.upper())\n",
    "\n",
    "dtypes = {\n",
    "         'BEST': 'string',\n",
    "         'ANR': 'string',  # Archivnummern erhalten entgegen des Namens manchmal Zeichen\n",
    "         'EAN': 'string',\n",
    "         'PEAN': 'string',\n",
    "         'LC': 'string',\n",
    "         'LN': 'string',\n",
    "         'BNR':'string',\n",
    "         'TTS': 'string',\n",
    "         'MIT': 'string',\n",
    "         'MIT_TYP': 'string',\n",
    "         'ABS_DAUER': 'string',\n",
    "         'T-ISRC': 'string',\n",
    "         'T-RHTI': 'string',\n",
    "         'RHTI': 'string',\n",
    "         'TRÄGER': 'Int64',\n",
    "         'SEITE': 'Int64',\n",
    "         'TAKE': 'Int64',\n",
    "         'AMO_ID': 'string',\n",
    "         'KAT_GATT': 'string',\n",
    "         'TTYP': 'string',\n",
    "         'TPRIO': 'string',\n",
    "         'TITEL': 'string',\n",
    "         'T-TITEL': 'string'\n",
    "}\n",
    "\n",
    "for dtype in list(dtypes.keys()):\n",
    "    dtypes[f\"Q_{dtype}\"] = dtypes[dtype]\n",
    "\n",
    "pd_args = dict(encoding='latin1',\n",
    "               sep=';',\n",
    "               na_values=na_values,\n",
    "               low_memory=False,\n",
    "               dtype=dtypes,\n",
    "               usecols=lambda x: 'l_' not in x and 'unnamed' not in x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabedaten lesen und in besser händelbare Form \n",
    "# bringen, d.h. eine CSV per Standort \n",
    "# (wie von `location_data` erwartet)\n",
    "\n",
    "input_dirs = [dir for dir in input.iterdir() if dir.is_dir()]\n",
    "\n",
    "every_location_has_dir = all(dir.name in locations for dir in input_dirs)\n",
    "\n",
    "if not every_location_has_dir:\n",
    "    raise Exception(f\"Data not as expected in {input}\")\n",
    "\n",
    "for dir in input_dirs:\n",
    "    location = dir.name\n",
    "    csv_files = [f for f in dir.glob('**/*') if f.is_file() and f.suffix == '.csv']\n",
    "    src = None\n",
    "    dst = f\"{workdir/location}.csv\"\n",
    "    tmp_src_csv = []\n",
    "    for csv_file in csv_files:\n",
    "        tmp_df = pd.read_csv(csv_file, index_col=1, **pd_args)\n",
    "        tmp_df['COVER_PDF'] = 'AMS' in csv_file.name and 'Mit_PDF' in csv_file.name\n",
    "        tmp_df['COVER_KEIN_PDF'] = 'AMS'  in csv_file.name and 'Ohne_PDF'  in csv_file.name\n",
    "        tmp_src_csv.append(tmp_df)\n",
    "    con_src_csv = pd.concat(tmp_src_csv)\n",
    "    con_src_csv.to_csv(dst, encoding='latin1', sep=';')\n",
    "    src_datei = dst\n",
    "    if src_datei == None:\n",
    "        raise Exception(f\"Couldn't collect data for {location}\")\n",
    "    print(f\"Collected {location} -> {dst}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "\n",
    "def get_data():\n",
    "    dfs = []\n",
    "    for location in locations:\n",
    "        src = f\"{workdir/location}.csv\"\n",
    "        tmp_df = pd.read_csv(src, index_col=0, **pd_args)\n",
    "        tmp_df.index = tmp_df.index.astype(str)\n",
    "        dfs.append(tmp_df)\n",
    "    data = pd.concat(dfs)\n",
    "\n",
    "    # Daten aufbereiten\n",
    "\n",
    "    # Bestand als Kategorie setzen \n",
    "    data['BEST'] = data['BEST'].astype('category')\n",
    "    data['BEST'] = data['BEST'].cat.set_categories(locations, ordered=True)\n",
    "\n",
    "    # EAN/PEAN zusammenfassen\n",
    "    data['EAN/PEAN'] = data['EAN']\n",
    "    data['EAN/PEAN'].fillna(data['PEAN'], inplace=True)\n",
    "\n",
    "\n",
    "    # Labelcode\n",
    "    data['LC'].fillna(data['LN'], inplace=True)\n",
    "\n",
    "    # Titel finden\n",
    "    data['RHTI'] = \"\"\n",
    "    data['T-RHTI'] = data['T-TITEL']\n",
    "    ## immer den RHTI mit höchster PRIO behalten\n",
    "    for prio in [\"3\", \"2\", \"1\"]:\n",
    "        mask = data['TPRIO'] == prio \n",
    "        data.loc[mask, 'RHTI'] = data.loc[mask, 'TITEL']\n",
    "\n",
    "\n",
    "    # MIT und MIT_TYP sind vertauscht\n",
    "    data['MIT'] = data['MIT_TYP']\n",
    "    # Es kann mehrere MIT geben, wobe die, die in \n",
    "    ## black_list_mit[0] sind nur mit Prio 2 genommen werden sollen und die in black_list_mit[3] nur mit Prio 3\n",
    "    # Bevorzugten MIT finden\n",
    "    def correct_mit(dd: pd.Series):\n",
    "         if(len(dd) < 2):\n",
    "              return dd.iloc[0]\n",
    "         has_prio_3_mit = dd.loc[dd.isin(black_list_mit[1])]\n",
    "         has_prio_2_mit = dd.loc[dd.isin(black_list_mit[0])]\n",
    "         has_prio_1_mit = dd.loc[~dd.isin([*black_list_mit[0], *black_list_mit[0]])]\n",
    "\n",
    "         if(len(has_prio_1_mit) > 0):\n",
    "              return has_prio_1_mit.iloc[0]\n",
    "         if(len(has_prio_2_mit) > 0):\n",
    "              return has_prio_2_mit.iloc[0]\n",
    "         if(len(has_prio_3_mit) > 0):\n",
    "              return has_prio_3_mit.iloc[0]\n",
    "         return dd.iloc[0]\n",
    "    data['MIT'] = data['MIT'].groupby('ANR').apply(correct_mit)\n",
    "\n",
    "\n",
    "    data.to_csv('rohdaten.csv')\n",
    "    print(f\"Total before deduplication: {len(data)}\")\n",
    "    # Duplikat-ANRs entfernen (durch mehrere Titel, MIT etc. entstanden)\n",
    "    data = data[~data.index.duplicated(keep='first')]\n",
    "    print(f\"Total after deduplication: {len(data)} \\n\")\n",
    "    data.to_csv('putzdaten.csv')\n",
    "\n",
    "    # Irrelevante Spalten entfernen\n",
    "    cols_to_delete = ['TTS', 'DAUER', 'ABS_DAUER', 'TPRIO', 'TITEL', \n",
    "                      'T-TITEL', 'MIT_TYP', 'LN', 'PEAN', 'EAN',\n",
    "                      'TTYP', 'KAT_GATT']\n",
    "    deleted_cols = []\n",
    "    for col_to_delete in cols_to_delete:\n",
    "        del data[col_to_delete]\n",
    "        deleted_cols.append(col_to_delete)\n",
    "        q_col = f\"Q_{col_to_delete}\"\n",
    "        if q_col in data:\n",
    "            del data[q_col]\n",
    "            deleted_cols.append(q_col)\n",
    "\n",
    "    print(f\"Deleted cols: {deleted_cols} \\n\")        \n",
    "    cols = [col for col in list(data.columns)]\n",
    "    print(f\"Kept cols: {cols} \\n\")\n",
    "    \n",
    "    # Plausibilisieren, dass MIT und RHTI korrekt gesetzt sind\n",
    "\n",
    "    assert data.loc['1254061']['RHTI'] == 'Stark'\n",
    "    assert data.loc['1254061']['MIT'] == 'Feller, Linda'\n",
    "\n",
    "    return data\n",
    "\n",
    "data = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wertverteilung visualisieren\n",
    "missing_values_dir = output / Path(\"fehlende_werte\") \n",
    "missing_values_dir.mkdir(exist_ok=True)\n",
    "for location in locations:\n",
    "    cols = data.columns\n",
    "    has_value = [ data.loc[data['BEST'] == location, col].count() for col in cols]\n",
    "    no_value = [ data.loc[data['BEST'] == location, col].isnull().sum() for col in cols]\n",
    "    compare_cols = pd.DataFrame({'fehlt': no_value, 'vorhanden': has_value}, index=cols)\n",
    "    ax = compare_cols.plot(kind='barh', stacked=True, title=f'{location}: Fehlende Daten')\n",
    "    ax.set(xlabel='Einheiten', ylabel='Spalten')\n",
    "    plt.title(f'{location}: Fehlende Werte')\n",
    "    plt.savefig(f\"{missing_values_dir / location}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeadb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion, um Vergleiche hinzuzufügen\n",
    "\n",
    "ALLOWED_CONDITIONS = [\n",
    "    'ignore_minimal_distance', # ndw: Schließe Dublette auch ein, wenn der Mindestabstand nicht erfüllt ist\n",
    "    'no_tst', # tmf: Träger, Seite, Take haben keinen Wert\n",
    "    'minimal_bnr', # BNR muss eine Mindestlänge (siehe unten) erfüllen \n",
    "    'shorten_z_bnr', # Kürze erste Stelle Z-BNR, wenn Z-BNR aus 6 oder mehr Ziffern besteht\n",
    "    'no_bnr', # bnf: BNR fehlt,\n",
    "    'check_q_locators'\n",
    "]\n",
    "\n",
    "LOCALISATORS = ['TRÄGER', 'SEITE', 'TAKE']\n",
    "\n",
    "# Spalten, die später hinzugefügt werden\n",
    "ADDITIONAL_COLS = ['Z-BNR']\n",
    "\n",
    "\n",
    "\n",
    "# Vergleich hinzufügen\n",
    "def compare(df: pd.DataFrame, to_compare: dict, use_cols: list, name_prefix = None, check_q_compare = True, use_conditions: typing.Dict[str, bool] = dict(), name_suffix: typing.Optional[str] = None) -> None:\n",
    "    name = \", \".join(use_cols)\n",
    "    if name_prefix is not None:\n",
    "        name = f'{name_prefix}: {name}'\n",
    "    if name_suffix is not None:\n",
    "        name = f'{name} {name_suffix}'\n",
    "    \n",
    "    duplicated_comparision = any(set(use_cols) == set(comparision['cols']) for comparision in to_compare.values())\n",
    "    if duplicated_comparision:\n",
    "        raise Exception(f'Comparision already included: {name}')\n",
    "        \n",
    "    unknown_cols = [col for col in use_cols if col not in [*df.columns, *ADDITIONAL_COLS]]\n",
    "    if len(unknown_cols) > 0:\n",
    "        raise Exception(f'Column(s) ({unknown_cols}) unknown.')\n",
    "    \n",
    "    unknown_conds = [cond for cond in use_conditions if cond not in ALLOWED_CONDITIONS]\n",
    "    if len(unknown_conds) > 0:\n",
    "        raise Exception(f'Condition(s) ({unknown_conds}) unknown.')\n",
    "    \n",
    "    \n",
    "   \n",
    "    to_compare[name] = dict()\n",
    "    to_compare[name]['cols'] = use_cols\n",
    "    to_compare[name]['conditions'] = use_conditions\n",
    "    print(f\"Added: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26374529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vergleiche hinzufügen\n",
    "to_compare = dict()\n",
    "\n",
    "tst = ['TRÄGER', 'SEITE', 'TAKE']\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['AMO_ID'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC', 'Z-BNR', 'MIT', 'RHTI', 'T-ISRC', 'T-RHTI'],\n",
    "        name_prefix=\"alles\",\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'Z-BNR', 'T-ISRC'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['EAN/PEAN', 'T-ISRC'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['RHTI', 'T-ISRC'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC', 'Z-BNR', 'MIT', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'T-RHTI', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC', 'Z-BNR', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['EAN/PEAN', 'LC', 'Z-BNR', 'RHTI'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['EAN/PEAN', 'LC', 'Z-BNR'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "# Tübinger Methode\n",
    "compare(data, to_compare,\n",
    "        ['MIT', 'LC', *tst, 'Z-BNR'],\n",
    "        name_prefix=\"Tübinger Methode\",\n",
    "        use_conditions=dict(shorten_z_bnr=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'LC', 'Z-BNR', 'T-RHTI'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'Z-BNR', 'T-RHTI'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'Z-BNR', 'RHTI'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'LC', 'Z-BNR', 'RHTI'],\n",
    "        use_conditions=dict(minimal_bnr=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'MIT', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['RHTI', 'T-RHTI'],\n",
    "        use_conditions=dict(no_tst=True,\n",
    "        no_bnr=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'MIT', 'RHTI'],\n",
    "        use_conditions=dict( no_tst=True,\n",
    "        no_bnr=True))\n",
    "\n",
    "compare(data, to_compare, \n",
    "        ['LC', 'RHTI', 'T-RHTI'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca15668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion: Vergleich vorbereiten\n",
    "\n",
    "PATTERN_NON_ALPHANUMERIC = re.compile(r'\\W+')\n",
    "PATTERN_ONLY_NUMBERS = re.compile(r'\\D')\n",
    "MINIMAL_DISTANCE = 80\n",
    "MINIMAL_BNR_LEN = 4\n",
    "minimal_distance_col = f'distance >= {MINIMAL_DISTANCE}?'\n",
    "    \n",
    "def prepare_data(data: pd.DataFrame):\n",
    "    PATTERN_NON_ALPHANUMERIC = re.compile(r'\\W+')\n",
    "    PATTERN_ONLY_NUMBERS = re.compile(r'\\D')\n",
    "    MINIMAL_DISTANCE = 80\n",
    "    MINIMAL_BNR_LEN = 4\n",
    "    minimal_distance_col = f'distance >= {MINIMAL_DISTANCE}?'\n",
    "\n",
    "    data['hat Dubletten?'] = False\n",
    "    data['ist Dublette?'] = False\n",
    "    data[minimal_distance_col] = False\n",
    "    data['Original ANR'] = np.nan\n",
    "    data['Original BEST'] = np.nan\n",
    "    data['Original RHTI'] = np.nan\n",
    "    data['Fundmethode'] = np.nan\n",
    "    data['AMO geteilt'] = False\n",
    "    data['Abstand'] = np.nan\n",
    "    data['Abstand'] = data['Abstand'].astype('Int64')\n",
    "\n",
    "    data['MIT'] = data['MIT'].str.replace(PATTERN_NON_ALPHANUMERIC, '').str.lower()\n",
    "    data['RHTI'] = data['RHTI'].str.replace(PATTERN_NON_ALPHANUMERIC, '').str.lower()\n",
    "    data['T-RHTI'] = data['T-RHTI'].str.replace(PATTERN_NON_ALPHANUMERIC, '').str.lower()\n",
    "\n",
    "    print('Generate Z-BNR (numbers only)')\n",
    "    data['Z-BNR'] = data['BNR'].str.replace(PATTERN_ONLY_NUMBERS, '').str.lower()\n",
    "\n",
    "    # EAN/PEAN 1. Stelle ignorieren, wenn länger als 12 Zeichen\n",
    "    print('Shorten PEAN if necessary')\n",
    "    data['O-PEAN'] = data['EAN/PEAN']\n",
    "    mask = data['EAN/PEAN'].str.len() > 12\n",
    "    column_name = 'EAN/PEAN'\n",
    "    data.loc[mask, column_name] = data.loc[mask, column_name].apply(lambda v: v[1:13])\n",
    "    data['t_ANR'] = data.index\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion: Ergebnisverteilung visualisieren\n",
    "\n",
    "def visualize_results():\n",
    "    result_dir = output / Path(\"vergleich_ergebnis\") \n",
    "    result_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    x = data.loc[data['ist Dublette?'] == True, 'Fundmethode'].value_counts()\n",
    "    y =  x.index\n",
    "    ax = sns.barplot(x=x, y=y)\n",
    "    ax.set(ylabel='Reihenfolge: Vergleichsmethode', xlabel='Dubletten (abs.)')\n",
    "    plt.title('Fundmethoden nach Fundmenge')\n",
    "    save_to = result_dir / 'fundmethoden.svg'\n",
    "    plt.savefig(save_to, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    x = data.loc[(data['ist Dublette?'] == True), ['BEST', 'Original BEST']].value_counts().unstack().sort_values('BEST')\n",
    "    ax = x.plot(kind='bar')\n",
    "    plt.title('Wo stehen die Originale der Dubletten?')\n",
    "    ax.legend(title='Standort des Originals')\n",
    "    ax.set(ylabel='Originale', xlabel='Standort der Dublette')\n",
    "    save_to = result_dir / 'dubletten_originale.svg'\n",
    "    plt.savefig(save_to, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    x = data.loc[:, ['BEST', 'ist Dublette?']].value_counts()\n",
    "    ax = x.unstack(level=1).plot(kind='bar', stacked=True)\n",
    "    ax.legend(['Original', 'Dublette'])\n",
    "    plt.title('Dubletten nach Standort (bezogen auf die jeweilige Gesamtmenge)')\n",
    "    ax.set(ylabel='Einheiten', xlabel='Standort')\n",
    "    save_to = result_dir / 'dubletten_nach_standort.svg'\n",
    "    plt.savefig(save_to, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    for location in locations: \n",
    "        if(len(data.loc[data['BEST'] == location]) == 0):\n",
    "            print(location)\n",
    "            continue\n",
    "        x = data.loc[data['BEST'] == location, ['BEST', 'ist Dublette?']].value_counts(normalize=True) * 100\n",
    "        ax = x.unstack().plot(kind='bar', stacked=True)\n",
    "        ax.legend(['Original', 'Dublette'])\n",
    "        plt.title(f'{location}: Dubletten im Bestand (prozentual)')\n",
    "        ax.set(ylabel='Einheiten (%)', xlabel=None)\n",
    "        save_to = result_dir / f'{location}_dubletten_nach_standort.svg'\n",
    "        plt.savefig(save_to, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e418aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AKTUELL\n",
    "\n",
    "data = prepare_data(get_data())\n",
    "\n",
    "# Vergleich durchführen\n",
    "\n",
    "# Lauf 1: Nur physische Locations\n",
    "physical_locations = {\n",
    "        \"comparisons\": to_compare,\n",
    "        \"found\": dict(),\n",
    "}\n",
    "\n",
    "# Lauf 2: AMS + physische Locations \n",
    "ams_and_results = {\n",
    "        \"comparisons\": to_compare,\n",
    "        \"found\": dict(),\n",
    "}\n",
    "# Nur AMS-Daten\n",
    "ams_data = data[(data['BEST'] == AMS) == True]\n",
    "# Läufe\n",
    "runs = [physical_locations, ams_and_results]\n",
    "\n",
    "idx_originals = []\n",
    "\n",
    "for run_num, run in enumerate(runs, 1):\n",
    "    # Lauf konfigurieren\n",
    "\n",
    "    is_ams_run = run_num == 2\n",
    "    \n",
    "\n",
    "    # Vergleichsreihenfolge \n",
    "    print(\"Comparing...\")\n",
    "    print(\"Priority\")\n",
    "    comp_locations = locations if is_ams_run else [l for l in locations if l != AMS]\n",
    "    for i, location in enumerate(comp_locations):\n",
    "        print(f\"{i}. {location}\")\n",
    "    print('\\n')\n",
    "\n",
    "    # Ergebniszahlensicherung vorbereiten\n",
    "    run[\"found\"]['total'] = dict()\n",
    "    ignored = 'ignorierte Dubletten (mit weniger Kriterien anderes Original gefunden, zur Sicherheit verworfen (Ergebnisse ohne Abstandsprüfung!)'\n",
    "    run[\"found\"][ignored] = 0\n",
    "    run[\"found\"]['total']['total'] = 0\n",
    "    run[\"found\"]['total'][minimal_distance_col] = 0\n",
    "    run[\"found\"]['total']['marked'] = 0\n",
    "\n",
    "    # Vergleich starten\n",
    "    for position, method in enumerate(to_compare, 1):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Zu vergleichende Daten\n",
    "        run_data = data.drop(data[(data['BEST'] == AMS) == True].index)\n",
    "        \n",
    "        if not is_ams_run: \n",
    "            print(f\"dropping {AMS}\")\n",
    "            print(f\"before {len(data)}\")\n",
    "            print(f\"after {len(run_data)}\")\n",
    "\n",
    "        if is_ams_run: # Zu vergleichende Daten auf Originale aus vorherigem Lauf beschränken\n",
    "            data_originals_only = run_data[(run_data['ist Dublette?'] == False)]\n",
    "            print(f\"adding {AMS}\")\n",
    "            print(f\"before (originals only) {len(data_originals_only)}\")\n",
    "            run_data = ams_data.append(data_originals_only)\n",
    "            print(f\"after {len(run_data)}\")\n",
    "\n",
    "        run_data['AMS run?'] = is_ams_run\n",
    "\n",
    "\n",
    "        is_found_by_amo_id = position == 0\n",
    "        found_by_method = f'{position}: {method}' # Fundmethode\n",
    "        cols = to_compare[method]['cols'] # Zu vergleichende Spalten\n",
    "        conditions = to_compare[method]['conditions'] # Vergleichsbedingungen\n",
    "        # Vergleichskonfiguration ausgeben\n",
    "        print(f'\\n Comparing: : {found_by_method} ({position}/{len(to_compare)})')\n",
    "        print(f'-> Columns: {cols}')\n",
    "        print(f\"-> Conditions: {conditions}\")\n",
    " \n",
    "        # Nur die noch nicht erkannten Kandidaten vergleichen\n",
    "        # und mit einer Kopie der Daten arbeiten, um data unbeschädigt zu halten\n",
    "        comp_data = run_data[(run_data['ist Dublette?'] == False)].copy()\n",
    "        print(f\"Vergleichsdaten: #1 {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "        # Vor-Bedingungen anwenden\n",
    "        if 'minimal_bnr' in conditions:\n",
    "            comp_data = comp_data.drop( (comp_data[ ~(comp_data['BNR'].str.len() >= MINIMAL_BNR_LEN) ] ).index)\n",
    "        if 'no_bnr' in conditions:\n",
    "            comp_data = comp_data.drop( (comp_data[ ~(comp_data['BNR'].isnull()) ] ).index)\n",
    "        if 'shorten_z_bnr' in conditions:\n",
    "            mask = comp_data['Z-BNR'].str.len() >= 6\n",
    "            column_name = 'Z-BNR'\n",
    "            comp_data.loc[mask, column_name] = comp_data.loc[mask, column_name].apply(lambda v: v[1:])\n",
    "        \n",
    "        print(f\"Vergleichsdaten: #2 {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "        # Reihen mit leeren Werten ausstoßen\n",
    "        comp_data = comp_data.dropna(subset=cols)\n",
    "        print(f\"Vergleichsdaten: #3 {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "        # Vergleichsreihenfolge entsprechend `locations` sicherstellen\n",
    "        comp_data = comp_data.sort_values(['BEST'])\n",
    "        print(f\"Vergleichsdaten: #4 {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "\n",
    "\n",
    "     \n",
    "        # Dubletten und korrespondierende Originale finden\n",
    "        dubletten = comp_data.duplicated(subset=cols)\n",
    "        originale = comp_data.groupby(cols)['t_ANR'].transform('first').values\n",
    "        comp_data['ist Dublette?'] = dubletten\n",
    "        comp_data['Original ANR'] = originale\n",
    "\n",
    "        # Nach-Bedingungen anwenden\n",
    "        if 'no_tst' in conditions:\n",
    "            tst = ['TRÄGER', 'SEITE', 'TAKE']\n",
    "            comp_data = comp_data.drop( (comp_data[ ~(comp_data[tst].isnull().all('columns')) ] ).index)\n",
    "            print(f\"Vergleichsdaten: #5 {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "\n",
    "        # Schon als Originale in vorherigen Vergleichen registrierte Einheiten nicht als Dubletten zählen\n",
    "        if not is_ams_run: # im AMS-Lauf sollen diese trotzdem gezählt werden\n",
    "            run[\"found\"][ignored] += comp_data.loc[(comp_data['ist Dublette?'] == True) & (data['hat Dubletten?'] == True), 'ist Dublette?'].sum() \n",
    "            comp_data.loc[ (comp_data['ist Dublette?'] == True) & (data['hat Dubletten?'] == True), 'ist Dublette?'] = False\n",
    "\n",
    "   \n",
    "\n",
    "        if len(comp_data[comp_data['ist Dublette?'] == True]) > 0:\n",
    "\n",
    "            # Metadaten zum Original setzen\n",
    "            comp_data['Original BEST'] = data.loc[comp_data['Original ANR'], 'BEST'].values\n",
    "            comp_data['Original RHTI'] = data.loc[comp_data['Original ANR'], 'RHTI'].values\n",
    "            print(f\"Vergleichsdaten: #6  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "\n",
    "            # Nur noch Dubletten behalten\n",
    "            comp_data = comp_data.drop( comp_data[(comp_data['ist Dublette?'] == False)].index)\n",
    "            print(f\"Vergleichsdaten: #7  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "\n",
    "            # Duplikat aus AMS-Lauf entfernen, deren Original nicht im AMS ist\n",
    "            if is_ams_run:\n",
    "                # AMS-interne Dubletten ignorieren\n",
    "                print(f\"before removing ams-internal dubletten: {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "                to_drop = comp_data[\n",
    "                        (comp_data['AMS run?'] == True) &\n",
    "                        (comp_data['BEST'] == AMS) & \n",
    "                        (comp_data['ist Dublette?'] == True) & \n",
    "                        (comp_data['Original BEST'] == AMS)\n",
    "                    ].index\n",
    "                comp_data = comp_data.drop(to_drop)\n",
    "                print(f\"after removing ams-internal dubletten: {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "                print(f\"Vergleichsdaten: #8  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "                # Physische Standort-interne Dubletten ignorieren\n",
    "                print(f\"before removing physical location-internal dubletten: {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "                to_drop = comp_data[\n",
    "                        (comp_data['AMS run?'] == True) &\n",
    "                        (comp_data['BEST'] != AMS) & \n",
    "                        (comp_data['ist Dublette?'] == True) & \n",
    "                        (comp_data['Original BEST'] != AMS)\n",
    "                    ].index\n",
    "                comp_data.drop(to_drop, inplace=True)\n",
    "                print(f\"after removing physical location-internal dubletten: {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "                print(f\"Vergleichsdaten: #9  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "            \n",
    "            # Abstand zwischen Dubletten berechnen\n",
    "            comp_data['eigene ANR (numerisch)'] = pd.to_numeric(comp_data.index.str.replace(r'[^\\d]', '', regex=True))\n",
    "            comp_data['originale ANR (numerisch)'] = pd.to_numeric(comp_data['Original ANR'].str.replace(r'[^\\d]', '', regex=True))\n",
    "            comp_data['Abstand'] = abs(comp_data['eigene ANR (numerisch)'] - comp_data['originale ANR (numerisch)'])\n",
    "            print(f\"Vergleichsdaten: #10  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "            # Gefundene Dubletten mit Mindestabstand in Ergebnisdaten schreiben\n",
    "            data.loc[comp_data.index, 'ist Dublette?'] = comp_data['Abstand'] >= MINIMAL_DISTANCE\n",
    "            print(f\"Vergleichsdaten: #11  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "            if ('ignore_minimal_distance' in conditions) or is_ams_run:\n",
    "                # Wenn Minimaldistanz ignoriert werden soll oder AMS-Lauf\n",
    "                # alle Dubletten nehmen\n",
    "                data.loc[comp_data.index, 'ist Dublette?'] = True\n",
    "            print(f\"Vergleichsdaten: #12  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "            # Dubletten bzw. Original-Metadaten in Ergebnisdaten schreiben\n",
    "            data.loc[comp_data.index, 'Original BEST'] = comp_data['Original BEST']\n",
    "            data.loc[comp_data.index, 'Original ANR'] = comp_data['Original ANR']\n",
    "            data.loc[comp_data.index, 'Original RHTI'] = comp_data['Original RHTI']\n",
    "            data.loc[data.index.isin(comp_data['Original ANR']), 'hat Dubletten?'] = True\n",
    "            data.loc[comp_data.index, 'Fundmethode'] = found_by_method\n",
    "            if is_found_by_amo_id:\n",
    "                data.loc[comp_data.index, 'AMO geteilt'] = True\n",
    "            data.loc[comp_data.index, 'Abstand'] = comp_data['Abstand']\n",
    "            data.loc[comp_data.index, minimal_distance_col] =  comp_data['Abstand'] >= MINIMAL_DISTANCE\n",
    "            print(f\"Vergleichsdaten: #13  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "        print(f\"Vergleichsdaten: #14  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "        # Vergleich-Ergebnisse speichern\n",
    "        found_total = 0\n",
    "        found_marked = 0\n",
    "        found_without_too_near = 0\n",
    "\n",
    "        has_found_something = len(comp_data[comp_data['ist Dublette?'] == True]) > 0\n",
    "        if has_found_something:\n",
    "            print(f\"Vergleichsdaten: #15  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "            found_total = len(comp_data['ist Dublette?'])\n",
    "            found_marked = (data.loc[comp_data.index, 'ist Dublette?']).sum()\n",
    "            found_without_too_near = len(comp_data[(comp_data['ist Dublette?'] == True) & (comp_data['Abstand'] >= MINIMAL_DISTANCE)])\n",
    "        print(f\"Vergleichsdaten: #16  {len(data[(data['hat Dubletten?'] == True) ])}\")\n",
    "        print(f'found (total): {found_total}')\n",
    "        print(f'found (excluding too near): {found_without_too_near}')\n",
    "        print(f'found (marked as dublette): {found_marked}\\n')\n",
    "        \n",
    "        run[\"found\"][found_by_method] = dict()\n",
    "        run[\"found\"][found_by_method]['total'] = found_total\n",
    "        run[\"found\"][found_by_method]['marked'] = found_marked\n",
    "        run[\"found\"][found_by_method][minimal_distance_col] = found_without_too_near\n",
    "        run[\"found\"]['total']['total'] += run[\"found\"][found_by_method]['total']\n",
    "        run[\"found\"]['total']['marked'] += run[\"found\"][found_by_method]['marked']\n",
    "        run[\"found\"]['total'][minimal_distance_col] += run[\"found\"][found_by_method][minimal_distance_col]\n",
    "        \n",
    "        comp_originals = comp_data[comp_data['hat Dubletten?']]['t_ANR'].to_list()\n",
    "        idx_originals = [*idx_originals, *comp_originals]\n",
    "        del comp_data \n",
    "    \n",
    "    if not is_ams_run:\n",
    "        assert len(data[ (data['ist Dublette?'] == True) & (data.index.isin(data['Original ANR'])) ]) == 0\n",
    "        assert len(data[ (data['ist Dublette?'] == True) & (data['hat Dubletten?'] == True) ]) == 0\n",
    "            \n",
    "    ipd.display(run[\"found\"]['total'])\n",
    "\n",
    "del data['t_ANR']\n",
    "print('Fertig!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801af001",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Matrix\n",
    "for run in runs:\n",
    "    g = run['found'].copy()\n",
    "\n",
    "    del g['total']\n",
    "    del g[ignored]\n",
    "\n",
    "    for m in g:\n",
    "        i = g[m]['total']\n",
    "        a = g[m][minimal_distance_col]\n",
    "        print(f'{i},{a}')\n",
    "\n",
    "    print('\\n')\n",
    "    for m in g:\n",
    "        i = g[m]['total']\n",
    "        a = g[m][minimal_distance_col]\n",
    "        print(f'{m}: {i},{a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9ad0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = data.copy()\n",
    "bool_cols = list()\n",
    "for col, col_type in dict(out.dtypes).items():\n",
    "    if col_type == 'bool':\n",
    "        out[col] = out[col].astype('object')\n",
    "        out.loc[(out[col] == True), col] = 'x'\n",
    "        out.loc[(out[col] == False), col] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = output / Path(f'ergebnis.xlsx') \n",
    "with pd.ExcelWriter(result_file) as writer:\n",
    "    out.to_excel(writer, sheet_name='Alles')\n",
    "    for location in locations:\n",
    "        out.loc[out['BEST'] == location, :].to_excel(writer, sheet_name=f'{location} Alles')\n",
    "        out.loc[out['BEST'] == location, :].sample(10).to_excel(writer, sheet_name=f'{location} Random sample (10)')\n",
    "        out.loc[(out['BEST'] == location) & (out['ist Dublette?'] == 'x'), :].to_excel(writer, sheet_name=f'{location} Dubletten')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fccddf5f6657516a9bd7193b80ac1bf94db59e33f3d9a8bee643279f0e1f271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
