{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c550a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import typing\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import IPython.display as ipd                               \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# Konfiguration\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Pfade aufsetzen\n",
    "output = Path(f\"output\")\n",
    "output.mkdir(exist_ok=True)\n",
    "input = Path('input')\n",
    "if not input.exists() or next(input.iterdir(), None) is None:\n",
    "    raise Exception(f\"Expected input data in {input.name} not found\")\n",
    "workdir = Path(\"workdir\")\n",
    "if workdir.is_dir():\n",
    "    shutil.rmtree(workdir)\n",
    "workdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Standorte; Reihenfolge nach Priorität \n",
    "# d.h. Originale werden immer aus dem Standort\n",
    "# genommen, der hier zuerst steht\n",
    "locations = ['AMS', 'MZ', 'STG', 'BAD',  'KA', 'TUE', 'MA', 'FR']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguration: CSV einlesen\n",
    "\n",
    "na_values = [\n",
    "    '0\\'00000\"',\n",
    "    '',\n",
    "    'UNBEKANNT',\n",
    "    'Unbekannt',\n",
    "    '-',\n",
    "    'o. A',\n",
    "    'o.A.',\n",
    "    # BNR: Platzhalter-Werte\n",
    "    '[]',\n",
    "    '[-]', \n",
    "    '[unbekannt]',\n",
    "    '[PROMO]',\n",
    "    '[- PROMO]',\n",
    "    '[[ PROMO]]',\n",
    "    '[ PROMO]',\n",
    "    '[Promo]',\n",
    "    '[Promo-CD]',\n",
    "    '[o. A.]',\n",
    "    '[ohne Nummer]',\n",
    "    '[ohne Nr.]',\n",
    "    '[o. Nr.]'\n",
    "    # RHTI: Leere Werte\n",
    "    ' \"',\n",
    "    ' \" ',\n",
    "    # MIT: Leere Werte\n",
    "    'Diverse',\n",
    "    'Nicht genannt',\n",
    "    'nicht genannt'\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Sonstige, überdurchschnittlich häufige Werte, ggf. auch als leer interpretieren?\n",
    "    # RHTI: Allweltstitel \n",
    "    'Live',\n",
    "    'Streichquartette',\n",
    "    'Lieder',\n",
    "    'Kammermusik',\n",
    "    'Live USA',\n",
    "    'Greatest hits', \n",
    "    'Greatest Hits',\n",
    "    'Die großen Erfolge',\n",
    "    # MIT: Gruppenbezeichnungen\n",
    "    'Diverse',\n",
    "    'Ensemble',\n",
    "    'Orchester',\n",
    "    'Original Cast'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for value in list(na_values):\n",
    "    na_values.append(value.lower())\n",
    "    na_values.append(value.upper())\n",
    "\n",
    "dtypes = {\n",
    "         'BEST': 'string',\n",
    "         'ANR': 'string',  # Archivnummern erhalten entgegen des Namens manchmal Zeichen\n",
    "         'EAN': 'string',\n",
    "         'PEAN': 'string',\n",
    "         'LC': 'string',\n",
    "         'LN': 'string',\n",
    "         'BNR':'string',\n",
    "         'TTS': 'string',\n",
    "         'MIT': 'string',\n",
    "         'MIT_TYP': 'string',\n",
    "         'ABS_DAUER': 'string',\n",
    "         'T-ISRC': 'string',\n",
    "         'T-RHTI': 'string',\n",
    "         'RHTI': 'string',\n",
    "         'TRÄGER': 'Int64',\n",
    "         'SEITE': 'Int64',\n",
    "         'TAKE': 'Int64',\n",
    "         'AMO_ID': 'string'\n",
    "}\n",
    "\n",
    "for dtype in list(dtypes.keys()):\n",
    "    dtypes[f\"Q_{dtype}\"] = dtypes[dtype]\n",
    "\n",
    "pd_args = dict(encoding='latin1',\n",
    "               sep=';',\n",
    "               na_values=na_values,\n",
    "               low_memory=False,\n",
    "               dtype=dtypes,\n",
    "               usecols=lambda x: 'l_' not in x and 'unnamed' not in x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabedaten lesen und in besser händelbare Form \n",
    "# bringen, d.h. eine CSV per Standort \n",
    "# (wie von `location_data` erwartet)\n",
    "\n",
    "input_dirs = [dir for dir in input.iterdir() if dir.is_dir()]\n",
    "\n",
    "every_location_has_dir = all(dir.name in locations for dir in input_dirs)\n",
    "\n",
    "if not every_location_has_dir:\n",
    "    raise Exception(f\"Data not as expected in {input}\")\n",
    "\n",
    "for dir in input_dirs:\n",
    "    location = dir.name\n",
    "    csv_files = [f for f in dir.glob('**/*') if f.is_file() and f.suffix == '.csv']\n",
    "    src = None\n",
    "    dst = f\"{workdir/location}.csv\"\n",
    "    tmp_src_csv = []\n",
    "    for csv_file in csv_files:\n",
    "        tmp_df = pd.read_csv(csv_file, index_col=1, **pd_args)\n",
    "        tmp_df['COVER_PDF'] = 'AMS' in csv_file.name and 'Mit_PDF' in csv_file.name\n",
    "        tmp_df['COVER_KEIN_PDF'] = 'AMS'  in csv_file.name and 'Ohne_PDF'  in csv_file.name\n",
    "        tmp_src_csv.append(tmp_df)\n",
    "    con_src_csv = pd.concat(tmp_src_csv)\n",
    "    con_src_csv.to_csv(dst, encoding='latin1', sep=';')\n",
    "    src_datei = dst\n",
    "    if src_datei == None:\n",
    "        raise Exception(f\"Couldn't collect data for {location}\")\n",
    "    print(f\"Collected {location} -> {dst}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "\n",
    "dfs = []\n",
    "for location in locations:\n",
    "    src = f\"{workdir/location}.csv\"\n",
    "    tmp_df = pd.read_csv(src, index_col=0, **pd_args)\n",
    "    tmp_df.index = tmp_df.index.astype(str)\n",
    "    dfs.append(tmp_df)\n",
    "data = pd.concat(dfs)\n",
    "\n",
    "# Daten aufbereiten\n",
    "\n",
    "# Bestand als Kategorie setzen \n",
    "data['BEST'] = data['BEST'].astype('category')\n",
    "data['BEST'] = data['BEST'].cat.set_categories(locations, ordered=True)\n",
    "\n",
    "# EAN/PEAN zusammenfassen\n",
    "data['EAN/PEAN'] = data['EAN']\n",
    "data['EAN/PEAN'].fillna(data['PEAN'], inplace=True)\n",
    "\n",
    "\n",
    "# Labelcode\n",
    "data['LC'].fillna(data['LN'], inplace=True)\n",
    "\n",
    "\n",
    "# Titel finden\n",
    "data['RHTI'] = \"\"\n",
    "data['T-RHTI'] = data['T-TITEL']\n",
    "## immer den RHTI mit höchster PRIO behalten\n",
    "for prio in [\"3\", \"2\", \"1\"]:\n",
    "    mask = data['TPRIO'] == prio \n",
    "    data.loc[mask, 'RHTI'] = data.loc[mask, 'TITEL']\n",
    "\n",
    "# MIT und MIT-TYP sind in den Eingangsdaten verwechselt\n",
    "data['MIT'] = data['MIT_TYP']\n",
    "\n",
    "\n",
    "print(f\"Total before deduplication: {len(data)}\")\n",
    "# Duplikat-ANRs entfernen (durch mehrere Titel, MIT etc. entstanden)\n",
    "data = data[~data.index.duplicated(keep='first')]\n",
    "print(f\"Total after deduplication: {len(data)} \\n\")\n",
    "\n",
    "# Irrelevante Spalten entfernen\n",
    "cols_to_delete = ['TTS', 'DAUER', 'ABS_DAUER', 'TPRIO', 'TITEL', \n",
    "                  'T-TITEL', 'MIT_TYP', 'LN', 'PEAN', 'EAN',\n",
    "                  'TTYP', 'KAT_GATT']\n",
    "deleted_cols = []\n",
    "for col_to_delete in cols_to_delete:\n",
    "    del data[col_to_delete]\n",
    "    deleted_cols.append(col_to_delete)\n",
    "    q_col = f\"Q_{col_to_delete}\"\n",
    "    if q_col in data:\n",
    "        del data[q_col]\n",
    "        deleted_cols.append(q_col)\n",
    "\n",
    "print(f\"Deleted cols: {deleted_cols} \\n\")        \n",
    "cols = [col for col in list(data.columns)]\n",
    "print(f\"Kept cols: {cols} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wertverteilung visualisieren\n",
    "\n",
    "missing_values_dir = output / Path(\"fehlende_werte\") \n",
    "missing_values_dir.mkdir(exist_ok=True)\n",
    "for location in locations:\n",
    "    cols = data.columns\n",
    "    has_value = [ data.loc[data['BEST'] == location, col].count() for col in cols]\n",
    "    no_value = [ data.loc[data['BEST'] == location, col].isnull().sum() for col in cols]\n",
    "    compare_cols = pd.DataFrame({'fehlt': no_value, 'vorhanden': has_value}, index=cols)\n",
    "    ax = compare_cols.plot(kind='barh', stacked=True, title=f'{location}: Fehlende Daten')\n",
    "    ax.set(xlabel='Einheiten', ylabel='Spalten')\n",
    "    plt.title(f'{location}: Fehlende Werte')\n",
    "    plt.savefig(f\"{missing_values_dir / location}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeadb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion, um Vergleiche hinzuzufügen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ALLOWED_CONDITIONS = [\n",
    "    'ignore_minimal_distance', # ndw: Schließe Dublette auch ein, wenn der Mindestabstand nicht erfüllt ist\n",
    "    'no_tst', # tmf: Träger, Seite, Take haben keinen Wert\n",
    "    'minimal_bnr', # BNR muss eine Mindestlänge (siehe unten) erfüllen \n",
    "    'shorten_z_bnr', # Kürze erste Stelle Z-BNR, wenn Z-BNR aus 6 oder mehr Ziffern besteht\n",
    "    'no_bnr', # bnf: BNR fehlt,\n",
    "    'check_q_locators'\n",
    "]\n",
    "\n",
    "LOCALISATORS = ['TRÄGER', 'SEITE', 'TAKE']\n",
    "\n",
    "# Spalten, die später hinzugefügt werden\n",
    "ADDITIONAL_COLS = ['Z-BNR']\n",
    "\n",
    "\n",
    "\n",
    "# Vergleich hinzufügen\n",
    "def compare(df: pd.DataFrame, to_compare: dict, use_cols: list, name_prefix = None, check_q_compare = True, use_conditions: typing.Dict[str, bool] = dict(), name_suffix: typing.Optional[str] = None) -> None:\n",
    "    name = \", \".join(use_cols)\n",
    "    if name_prefix is not None:\n",
    "        name = f'{name_prefix}: {name}'\n",
    "    if name_suffix is not None:\n",
    "        name = f'{name} {name_suffix}'\n",
    "    \n",
    "    if check_q_compare and any(col in LOCALISATORS for col in use_cols):\n",
    "        return add_q_compare(df, to_compare, use_cols, name_prefix, check_q_compare=False, use_conditions=use_conditions)\n",
    "\n",
    "    duplicated_comparision = any(set(use_cols) == set(comparision['cols']) for comparision in to_compare.values())\n",
    "    if duplicated_comparision:\n",
    "        raise Exception(f'Comparision already included: {name}')\n",
    "        \n",
    "    unknown_cols = [col for col in use_cols if col not in [*df.columns, *ADDITIONAL_COLS]]\n",
    "    if len(unknown_cols) > 0:\n",
    "        raise Exception(f'Column(s) ({unknown_cols}) unknown.')\n",
    "    \n",
    "    unknown_conds = [cond for cond in use_conditions if cond not in ALLOWED_CONDITIONS]\n",
    "    if len(unknown_conds) > 0:\n",
    "        raise Exception(f'Condition(s) ({unknown_conds}) unknown.')\n",
    "    \n",
    "    \n",
    "   \n",
    "    to_compare[name] = dict()\n",
    "    to_compare[name]['cols'] = use_cols\n",
    "    to_compare[name]['conditions'] = use_conditions\n",
    "    print(f\"Added: {name}\")\n",
    "\n",
    "\n",
    "\n",
    "# Q-Elemente ebenfalls berücksichtigen\n",
    "def add_q_compare(df: pd.DataFrame, to_compare: dict, use_cols: list, name_prefix = None, check_q_compare = True, use_conditions: typing.Dict[str, bool] = dict()) -> None:\n",
    "    \n",
    "    localisator_cols = []\n",
    "    final_cols = []\n",
    "\n",
    "    for col in use_cols:\n",
    "        if col in LOCALISATORS:\n",
    "            localisator_cols.append(col)\n",
    "            localisator_cols.append(f'Q_{col}')\n",
    "        else:\n",
    "            final_cols.append(col)\n",
    "        \n",
    "    def is_proper_combination(combination: tuple):\n",
    "        return any(\"TRÄGER\" in s for s in combination) and any(\"SEITE\" in s for s in combination) and any(\"TAKE\" in s for s in combination)\n",
    "\n",
    "    new_compares = [[*final_cols, *combination] for combination in itertools.combinations(localisator_cols, 3) if is_proper_combination(combination)]\n",
    "    for i, cols in enumerate(new_compares, 1):\n",
    "        \n",
    "        compare(df, to_compare, use_cols=cols, name_prefix=name_prefix, check_q_compare = False, use_conditions=use_conditions, name_suffix=f'Kombination ({i})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26374529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vergleiche hinzufügen\n",
    "\n",
    "\n",
    "to_compare = dict()\n",
    "\n",
    "tst = ['TRÄGER', 'SEITE', 'TAKE']\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['AMO_ID'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC', 'Z-BNR', 'MIT', 'RHTI', 'T-ISRC', 'T-RHTI'],\n",
    "        name_prefix=\"alles\",\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'Z-BNR', 'T-ISRC'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['EAN/PEAN', 'T-ISRC'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['RHTI', 'T-ISRC'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC', 'Z-BNR', 'MIT', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'T-RHTI', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC', 'Z-BNR', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['EAN/PEAN', 'LC', 'Z-BNR', 'RHTI'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['EAN/PEAN', 'LC', 'Z-BNR'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "# Tübinger Methode\n",
    "compare(data, to_compare,\n",
    "        ['MIT', 'LC', *tst, 'Z-BNR'],\n",
    "        name_prefix=\"Tübinger Methode\",\n",
    "        use_conditions=dict(shorten_z_bnr=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'EAN/PEAN', 'LC'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'LC', 'Z-BNR', 'T-RHTI'],\n",
    "        use_conditions=dict(ignore_minimal_distance=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'Z-BNR', 'T-RHTI'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'Z-BNR', 'RHTI'],\n",
    "        use_conditions=dict(no_tst=True))\n",
    "\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'LC', 'Z-BNR', 'RHTI'],\n",
    "        use_conditions=dict(minimal_bnr=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        [*tst, 'MIT', 'RHTI'])\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['RHTI', 'T-RHTI'],\n",
    "        use_conditions=dict(no_tst=True,\n",
    "        no_bnr=True))\n",
    "\n",
    "compare(data, to_compare,\n",
    "        ['LC', 'MIT', 'RHTI'],\n",
    "        use_conditions=dict( no_tst=True,\n",
    "        no_bnr=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca15668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich vorbereiten\n",
    "\n",
    "PATTERN_NON_ALPHANUMERIC = re.compile(r'\\W+')\n",
    "PATTERN_ONLY_NUMBERS = re.compile(r'\\D')\n",
    "MINIMAL_DISTANCE = 80\n",
    "MINIMAL_BNR_LEN = 4\n",
    "minimal_distance_col = f'distance >= {MINIMAL_DISTANCE}?'\n",
    "\n",
    "data['hat Dubletten?'] = False\n",
    "data['ist Dublette?'] = False\n",
    "data[minimal_distance_col] = False\n",
    "data['Original ANR'] = np.nan\n",
    "data['Original BEST'] = np.nan\n",
    "data['Original RHTI'] = np.nan\n",
    "data['Fundmethode'] = np.nan\n",
    "data['AMO geteilt'] = False\n",
    "data['Abstand'] = np.nan\n",
    "data['Abstand'] = data['Abstand'].astype('Int64')\n",
    "\n",
    "data['MIT'] = data['MIT'].str.replace(PATTERN_NON_ALPHANUMERIC, '').str.lower()\n",
    "data['RHTI'] = data['RHTI'].str.replace(PATTERN_NON_ALPHANUMERIC, '').str.lower()\n",
    "data['T-RHTI'] = data['T-RHTI'].str.replace(PATTERN_NON_ALPHANUMERIC, '').str.lower()\n",
    "\n",
    "print('Generate Z-BNR (numbers only)')\n",
    "data['Z-BNR'] = data['BNR'].str.replace(PATTERN_ONLY_NUMBERS, '').str.lower()\n",
    "\n",
    "# EAN/PEAN 1. Stelle ignorieren, wenn länger als 12 Zeichen\n",
    "print('Shorten PEAN if necessary')\n",
    "data['O-PEAN'] = data['EAN/PEAN']\n",
    "mask = data['EAN/PEAN'].str.len() > 12\n",
    "column_name = 'EAN/PEAN'\n",
    "data.loc[mask, column_name] = data.loc[mask, column_name].apply(lambda v: v[1:13])\n",
    "data['t_ANR'] = data.index\n",
    "\n",
    "# Vergleich durchführen\n",
    "\n",
    "\n",
    "print(\"Comparing...\")\n",
    "print(\"Priority\")\n",
    "for i, location in enumerate(locations):\n",
    "    print(f\"{i}. {location}\")\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "found = dict()\n",
    "found['total'] = dict()\n",
    "ignored = 'ignorierte Dubletten (mit weniger Kriterien anderes Original gefunden, zur Sicherheit verworfen (Ergebnisse ohne Abstandsprüfung!)'\n",
    "found[ignored] = 0\n",
    "found['total']['total'] = 0\n",
    "found['total'][minimal_distance_col] = 0\n",
    "found['total']['marked'] = 0\n",
    "\n",
    "for position, method in enumerate(to_compare, 1):\n",
    "    fundmethod = f'{position}: {method}'\n",
    "    cols = to_compare[method]['cols']\n",
    "    conditions = to_compare[method]['conditions']\n",
    "    print(f'\\n Comparing: : {fundmethod} ({position}/{len(to_compare)})')\n",
    "    print(f'-> Columns: {cols}')\n",
    "    print(f\"-> Conditions: {conditions}\")\n",
    "    \n",
    "\n",
    "    vgl = data[(data['ist Dublette?'] == False)].copy()\n",
    "        \n",
    "    if 'minimal_bnr' in conditions:\n",
    "        vgl.drop( (vgl[ ~(vgl['BNR'].str.len() >= MINIMAL_BNR_LEN) ] ).index, inplace=True)\n",
    "    \n",
    "    \n",
    "    if 'no_bnr' in conditions:\n",
    "        vgl.drop( (vgl[ ~(vgl['BNR'].isnull()) ] ).index, inplace=True)\n",
    "        \n",
    "    if 'shorten_z_bnr' in conditions:\n",
    "        mask = vgl['Z-BNR'].str.len() >= 6\n",
    "        column_name = 'Z-BNR'\n",
    "        vgl.loc[mask, column_name] = vgl.loc[mask, column_name].apply(lambda v: v[1:])\n",
    "        \n",
    "    \n",
    "\n",
    "    fillNA_INT = 1337\n",
    "    fillna_STR = \"leerwert\"\n",
    "    for col in cols:\n",
    "        #get dtype for column\n",
    "        dtype = vgl[col].dtype \n",
    "        if dtype == \"Int64\":\n",
    "            vgl[col].fillna(fillNA_INT, inplace=True)\n",
    "        else:\n",
    "            vgl[col].fillna(fillna_STR, inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    vgl.sort_values(['BEST'], inplace=True)\n",
    "    \n",
    "\n",
    "    dubletten = vgl.duplicated(subset=cols)\n",
    "    originale = vgl.groupby(cols)['t_ANR'].transform('first').values\n",
    "    \n",
    "    vgl['ist Dublette?'] = dubletten\n",
    "    vgl['Original ANR'] = originale\n",
    "    \n",
    "    \n",
    "\n",
    "    for col in cols:\n",
    "        vgl = vgl.drop(vgl[(vgl[col] == fillNA_INT) | (vgl[col] == fillna_STR)].index)\n",
    "\n",
    "\n",
    "    if 'no_tst' in conditions:\n",
    "        tst_spalten = ['TRÄGER', 'SEITE', 'TAKE']\n",
    "        vgl.drop((vgl[(vgl[col] == fillNA_INT) | (vgl[col] == fillna_STR)]).index, \n",
    "                inplace=True)    \n",
    "\n",
    "    if len(vgl[vgl['ist Dublette?'] == True]) > 0:\n",
    "        # Schon als Originale registrierte Einheiten nicht als Dubletten zählen\n",
    "        found[ignored] += vgl.loc[(vgl['ist Dublette?'] == True) & (data['hat Dubletten?'] == True), 'ist Dublette?'].sum() \n",
    "        vgl.loc[ (vgl['ist Dublette?'] == True) & (data['hat Dubletten?'] == True), 'ist Dublette?'] = False\n",
    "\n",
    "\n",
    "        vgl['Original ANR'] = vgl['Original ANR']\n",
    "        vgl['Original BEST'] = data.loc[vgl['Original ANR'], 'BEST'].values\n",
    "        vgl['Original RHTI'] = data.loc[vgl['Original ANR'], 'RHTI'].values\n",
    "\n",
    "        vgl.drop( vgl[(vgl['ist Dublette?'] == False)].index, inplace=True)\n",
    "        \n",
    "        vgl['eigene ANR (numerisch)'] = pd.to_numeric(vgl.index.str.replace(r'[^\\d]', '', regex=True))\n",
    "        vgl['originale ANR (numerisch)'] = pd.to_numeric(vgl['Original ANR'].str.replace(r'[^\\d]', '', regex=True))\n",
    "        vgl['Abstand'] = abs(vgl['eigene ANR (numerisch)'] - vgl['originale ANR (numerisch)'])\n",
    "\n",
    "        # Ergebnisse übertragen\n",
    "        if 'ignore_minimal_distance' in conditions:\n",
    "             data.loc[vgl.index, 'ist Dublette?'] = True\n",
    "        else:\n",
    "            data.loc[vgl.index, 'ist Dublette?'] = vgl['Abstand'] >= MINIMAL_DISTANCE\n",
    "\n",
    "\n",
    "        data.loc[vgl.index, 'Original ANR'] = vgl['Original ANR']\n",
    "        data.loc[vgl.index, 'Original BEST'] = vgl['Original BEST']\n",
    "        data.loc[vgl.index, 'Original RHTI'] = vgl['Original RHTI']\n",
    "        data.loc[data.index.isin(vgl['Original ANR']), 'hat Dubletten?'] = True\n",
    "        data.loc[vgl.index, 'Fundmethode'] = fundmethod\n",
    "        if position == 0:\n",
    "            data.loc[vgl.index, 'AMO geteilt'] = True\n",
    "        data.loc[vgl.index, 'Abstand'] = vgl['Abstand']\n",
    "        data.loc[vgl.index, minimal_distance_col] =  vgl['Abstand'] >= MINIMAL_DISTANCE\n",
    "    \n",
    "    \n",
    "    found_total = 0\n",
    "    found_marked = 0\n",
    "    found_without_too_near = 0\n",
    "    if len(vgl[vgl['ist Dublette?'] == True]) > 0:\n",
    "        found_total = len(vgl['ist Dublette?'])\n",
    "        found_marked = (data.loc[vgl.index, 'ist Dublette?']).sum()\n",
    "        found_without_too_near = len(vgl[(vgl['ist Dublette?'] == True) & (vgl['Abstand'] >= MINIMAL_DISTANCE)])\n",
    "    \n",
    "    print(f'found (total): {found_total}')\n",
    "    print(f'found (excluding too near): {found_without_too_near}')\n",
    "    print(f'found (marked as dublette): {found_marked}\\n')\n",
    "    \n",
    "    found[fundmethod] = dict()\n",
    "    found[fundmethod]['total'] = found_total\n",
    "    found[fundmethod]['marked'] = found_marked\n",
    "    found[fundmethod][minimal_distance_col] = found_without_too_near\n",
    "    found['total']['total'] += found[fundmethod]['total']\n",
    "    found['total']['marked'] += found[fundmethod]['marked']\n",
    "    found['total'][minimal_distance_col] += found[fundmethod][minimal_distance_col]\n",
    "    \n",
    "    del vgl \n",
    "\n",
    "del data['t_ANR']\n",
    "\n",
    "assert len(data[ (data['ist Dublette?'] == True) & (data.index.isin(data['Original ANR'])) ]) == 0\n",
    "assert len(data[ (data['ist Dublette?'] == True) & (data['hat Dubletten?'] == True) ]) == 0\n",
    "           \n",
    "ipd.display(found['total'])\n",
    "print('Fertig!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Matrix\n",
    "\n",
    "g = found.copy()\n",
    "\n",
    "del g['total']\n",
    "del g[ignored]\n",
    "\n",
    "for m in g:\n",
    "    i = g[m]['total']\n",
    "    a = g[m][minimal_distance_col]\n",
    "    print(f'{i},{a}')\n",
    "\n",
    "print('\\n')\n",
    "for m in g:\n",
    "    i = g[m]['total']\n",
    "    a = g[m][minimal_distance_col]\n",
    "    print(f'{m}: {i},{a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierungen: Ergebnisverteilung\n",
    "\n",
    "result_dir = output / Path(\"vergleich_ergebnis\")\n",
    "result_dir.mkdir(exist_ok=True)\n",
    "\n",
    "x = data.loc[data['ist Dublette?'] == True, 'Fundmethode'].value_counts()\n",
    "y =  x.index\n",
    "ax = sns.barplot(x=x, y=y)\n",
    "ax.set(ylabel='Reihenfolge: Vergleichsmethode', xlabel='Dubletten (abs.)')\n",
    "plt.title('Fundmethoden nach Fundmenge')\n",
    "save_to = result_dir / 'fundmethoden.svg'\n",
    "plt.savefig(save_to, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "x = data.loc[(data['ist Dublette?'] == True), ['BEST', 'Original BEST']].value_counts().unstack().sort_values('BEST')\n",
    "ax = x.plot(kind='bar')\n",
    "plt.title('Wo stehen die Originale der Dubletten?')\n",
    "ax.legend(title='Standort des Originals')\n",
    "ax.set(ylabel='Originale', xlabel='Standort der Dublette')\n",
    "save_to = result_dir / 'dubletten_originale.svg'\n",
    "plt.savefig(save_to, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "x = data.loc[:, ['BEST', 'ist Dublette?']].value_counts()\n",
    "ax = x.unstack(level=1).plot(kind='bar', stacked=True)\n",
    "ax.legend(['Original', 'Dublette'])\n",
    "plt.title('Dubletten nach Standort (bezogen auf die jeweilige Gesamtmenge)')\n",
    "ax.set(ylabel='Einheiten', xlabel='Standort')\n",
    "save_to = result_dir / 'dubletten_nach_standort.svg'\n",
    "plt.savefig(save_to, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "for location in locations: \n",
    "    x = data.loc[data['BEST'] == location, ['BEST', 'ist Dublette?']].value_counts(normalize=True) * 100\n",
    "    ax = x.unstack().plot(kind='bar', stacked=True)\n",
    "    ax.legend(['Original', 'Dublette'])\n",
    "    plt.title(f'{location}: Dubletten im Bestand (prozentual)')\n",
    "    ax.set(ylabel='Einheiten (%)', xlabel=None)\n",
    "    save_to = result_dir / f'{location}_dubletten_nach_standort.svg'\n",
    "    plt.savefig(save_to, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis speichern\n",
    "\n",
    "results_file = output / Path(f'ergebnis.xlsx')\n",
    "if results_file.exists():\n",
    "    results_file.unlink()\n",
    "\n",
    "out = data.copy()\n",
    "bool_cols = list()\n",
    "for col, col_type in dict(out.dtypes).items():\n",
    "    if col_type == 'bool':\n",
    "        out[col] = out[col].astype('object')\n",
    "        out.loc[(out[col] == True), col] = 'x'\n",
    "        out.loc[(out[col] == False), col] = np.nan\n",
    "\n",
    "        \n",
    "with pd.ExcelWriter(results_file) as writer:\n",
    "    for location in locations:\n",
    "        out.to_excel(writer, sheet_name='Alles')\n",
    "        out.loc[out['BEST'] == location, :].to_excel(writer, sheet_name=f'{location} Alles')\n",
    "        out.loc[out['BEST'] == location, :].sample(10).to_excel(writer, sheet_name=f'{location} Random sample (10)')\n",
    "        out.loc[(out['BEST'] == location) & (out['ist Dublette?'] == 'x'), :].to_excel(writer, sheet_name=f'{location} Dubletten')\n",
    "\n",
    "del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c43a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18031972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "155266d56419206bfc728515faa99ba5cdd2ff0d7595a9346b367eb178ec549a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
